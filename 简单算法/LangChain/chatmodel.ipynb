{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "394b351e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acf58c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.chat_models import ChatTongyi\n",
    "\n",
    "chat = ChatTongyi()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6f1d8c",
   "metadata": {},
   "source": [
    "# é˜»å¡æ¨¡å¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4e77de4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='å‹¾è‚¡å®šç†ï¼ˆPythagorean Theoremï¼‰æ˜¯å‡ ä½•å­¦ä¸­çš„ä¸€ä¸ªåŸºæœ¬å®šç†ï¼Œæè¿°äº†ç›´è§’ä¸‰è§’å½¢ä¸‰è¾¹ä¹‹é—´çš„å…³ç³»ã€‚\\n\\n### å‹¾è‚¡å®šç†çš„è¡¨è¿°å¦‚ä¸‹ï¼š\\n\\nåœ¨**ç›´è§’ä¸‰è§’å½¢**ä¸­ï¼Œ**æ–œè¾¹**ï¼ˆå³ä¸ç›´è§’ç›¸å¯¹çš„è¾¹ï¼Œä¹Ÿå°±æ˜¯æœ€é•¿çš„è¾¹ï¼‰çš„å¹³æ–¹ç­‰äº**ä¸¤æ¡ç›´è§’è¾¹**ï¼ˆå³æ„æˆç›´è§’çš„ä¸¤æ¡è¾¹ï¼‰çš„å¹³æ–¹å’Œã€‚\\n\\næ•°å­¦è¡¨è¾¾å¼ä¸ºï¼š\\n\\n$$\\na^2 + b^2 = c^2\\n$$\\n\\nå…¶ä¸­ï¼š\\n- $ a $ å’Œ $ b $ æ˜¯ç›´è§’ä¸‰è§’å½¢çš„ä¸¤æ¡ç›´è§’è¾¹ï¼›\\n- $ c $ æ˜¯æ–œè¾¹ï¼ˆå³ç›´è§’å¯¹é¢çš„è¾¹ï¼‰ã€‚\\n\\n---\\n\\n### ä¸¾ä¾‹è¯´æ˜ï¼š\\n\\nå¦‚æœä¸€ä¸ªç›´è§’ä¸‰è§’å½¢çš„ä¸¤æ¡ç›´è§’è¾¹åˆ†åˆ«æ˜¯ 3 å’Œ 4ï¼Œé‚£ä¹ˆæ–œè¾¹ $ c $ å¯ä»¥é€šè¿‡å‹¾è‚¡å®šç†è®¡ç®—ï¼š\\n\\n$$\\nc^2 = 3^2 + 4^2 = 9 + 16 = 25 \\\\Rightarrow c = \\\\sqrt{25} = 5\\n$$\\n\\nè¿™å°±æ˜¯è‘—åçš„â€œ3-4-5â€ç›´è§’ä¸‰è§’å½¢ã€‚\\n\\n---\\n\\n### å†å²èƒŒæ™¯ï¼š\\n\\nå‹¾è‚¡å®šç†æœ€æ—©å¯ä»¥è¿½æº¯åˆ°å¤å·´æ¯”ä¼¦æ—¶æœŸï¼ˆçº¦å…¬å…ƒå‰1800å¹´ï¼‰ï¼Œä½†æœ€è‘—åçš„è®°è½½æ˜¯å‡ºç°åœ¨ä¸­å›½å¤ä»£ã€Šå‘¨é«€ç®—ç»ã€‹ä¸­ï¼Œä¹Ÿè¢«ç§°ä¸ºâ€œå•†é«˜å®šç†â€ã€‚åœ¨å¤å¸Œè…Šï¼Œæ•°å­¦å®¶**æ¯•è¾¾å“¥æ‹‰æ–¯**ï¼ˆPythagorasï¼‰å¯¹è¿™ä¸ªå®šç†è¿›è¡Œäº†ç³»ç»Ÿçš„ç ”ç©¶å’Œè¯æ˜ï¼Œå› æ­¤è¿™ä¸ªå®šç†è¢«åäººç§°ä¸ºâ€œå‹¾è‚¡å®šç†â€æˆ–â€œæ¯•è¾¾å“¥æ‹‰æ–¯å®šç†â€ã€‚\\n\\n---\\n\\n### åº”ç”¨ï¼š\\n\\nå‹¾è‚¡å®šç†å¹¿æ³›åº”ç”¨äºï¼š\\n- æ•°å­¦ã€ç‰©ç†ã€å·¥ç¨‹ã€å»ºç­‘ç­‰é¢†åŸŸçš„æµ‹é‡å’Œè®¡ç®—ï¼›\\n- è®¡ç®—ä¸¤ç‚¹ä¹‹é—´çš„ç›´çº¿è·ç¦»ï¼ˆåœ¨åæ ‡ç³»ä¸­ï¼‰ï¼›\\n- è§£å†³å„ç§å‡ ä½•é—®é¢˜ã€‚\\n\\n---\\n\\nå¦‚æœä½ éœ€è¦æ›´æ·±å…¥çš„è§£é‡Šã€è¯æ˜è¿‡ç¨‹æˆ–è€…åº”ç”¨å®ä¾‹ï¼Œæˆ‘å¯ä»¥ç»§ç»­ä¸ºä½ è¯¦ç»†è®²è§£ï¼', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': '3b00259c-a656-9a46-b72c-0273575bc1e1', 'token_usage': {'input_tokens': 26, 'output_tokens': 418, 'total_tokens': 444, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--4571964b-8332-46d9-9ca0-5fe58d1c7cf7-0')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "\n",
    "messages = [\n",
    "    SystemMessage(content=\"ä½ æ˜¯ä¸€ä¸ªæ•°å­¦ä¸“å®¶\"),\n",
    "    HumanMessage(content=\"ä»€ä¹ˆæ˜¯å‹¾è‚¡å®šç†\"),\n",
    "]\n",
    "\n",
    "chat.invoke(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d176aa48",
   "metadata": {},
   "source": [
    "# æµå¼"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c7f84f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å‹¾|è‚¡|å®š|ç†|ï¼ˆPythagorean| Theoremï¼‰æ˜¯|å‡ ä½•å­¦ä¸­çš„ä¸€ä¸ª|åŸºæœ¬å®šç†ï¼Œ|é€‚ç”¨äº**ç›´è§’|ä¸‰è§’å½¢**ã€‚|å®ƒçš„å†…å®¹æ˜¯ï¼š\n",
      "\n",
      "|> åœ¨ä¸€ä¸ª**|ç›´è§’ä¸‰è§’å½¢|**ä¸­ï¼Œ**|æ–œè¾¹**ï¼ˆ|å³ä¸ç›´è§’|ç›¸å¯¹çš„è¾¹ï¼Œ|ä¹Ÿå°±æ˜¯æœ€é•¿çš„è¾¹|ï¼‰çš„å¹³æ–¹ç­‰äº|å¦å¤–ä¸¤æ¡**ç›´|è§’è¾¹**çš„|å¹³æ–¹å’Œã€‚\n",
      "\n",
      "###| æ•°å­¦è¡¨è¾¾å¼|ï¼š\n",
      "å¦‚æœä¸€ä¸ªç›´|è§’ä¸‰è§’å½¢çš„|ä¸¤æ¡ç›´è§’è¾¹|åˆ†åˆ«ä¸º $ a $| å’Œ $ b $|ï¼Œæ–œè¾¹ä¸º| $ c $ï¼Œ|é‚£ä¹ˆæœ‰ï¼š\n",
      "\n",
      "$$|\n",
      "a^2| + b^2| = c^2|\n",
      "$$\n",
      "\n",
      "---\n",
      "\n",
      "|### ä¸¾ä¾‹è¯´æ˜|ï¼š\n",
      "\n",
      "æ¯”å¦‚ä¸€ä¸ªç›´|è§’ä¸‰è§’å½¢çš„|ä¸¤æ¡ç›´è§’è¾¹|åˆ†åˆ«æ˜¯ 3 å’Œ| 4ï¼Œé‚£ä¹ˆ|æ–œè¾¹ $ c| $ çš„é•¿åº¦å°±æ˜¯|ï¼š\n",
      "\n",
      "$$\n",
      "c| = \\sqrt{|3^2 +| 4^2|} = \\sqrt|{9 + |16} =| \\sqrt{2|5} = |5\n",
      "$$\n",
      "\n",
      "|è¿™å°±æ˜¯è‘—åçš„â€œ**|3-4-|5**â€ç›´|è§’ä¸‰è§’å½¢ã€‚\n",
      "\n",
      "|---\n",
      "\n",
      "###| å†å²èƒŒæ™¯ï¼š\n",
      "\n",
      "|å‹¾è‚¡å®šç†|ä»¥å¤å¸Œè…Šæ•°å­¦|å®¶**æ¯•è¾¾|å“¥æ‹‰æ–¯**ï¼ˆ|Pythagorasï¼‰|çš„åå­—å‘½åï¼Œä½†|äº‹å®ä¸Šï¼Œè¿™ä¸ªå®š|ç†åœ¨æ¯•è¾¾|å“¥æ‹‰æ–¯ä¹‹å‰å°±å·²ç»|è¢«å¤ä»£å·´æ¯”|ä¼¦äººã€åŸƒåŠ|äººå’Œä¸­å›½äººæ‰€|çŸ¥é“ã€‚åœ¨ä¸­å›½ï¼Œ|å®ƒè¢«ç§°ä¸º**å•†|é«˜å®šç†**|ï¼Œæœ€æ—©è§äº|ã€Šå‘¨é«€ç®—|ç»ã€‹ã€‚\n",
      "\n",
      "---\n",
      "\n",
      "|### åº”ç”¨|ï¼š\n",
      "\n",
      "å‹¾è‚¡å®š|ç†åœ¨æ•°å­¦ã€|ç‰©ç†ã€å·¥ç¨‹ã€|å»ºç­‘ã€å¯¼èˆªç­‰é¢†åŸŸ|éƒ½æœ‰å¹¿æ³›åº”ç”¨ï¼Œä¾‹å¦‚|ï¼š\n",
      "\n",
      "- è®¡|ç®—ä¸¤ç‚¹ä¹‹é—´çš„ç›´çº¿|è·ç¦»\n",
      "-| æµ‹é‡å»ºç­‘ç‰©çš„é«˜åº¦|æˆ–å®½åº¦\n",
      "-| è§£å†³ä¸‰è§’å‡½æ•°|é—®é¢˜\n",
      "- |ç”¨äºè®¡ç®—æœºå›¾å½¢å­¦|å’Œä¸‰ç»´ç©ºé—´è®¡ç®—|\n",
      "\n",
      "---\n",
      "\n",
      "å¦‚æœä½ éœ€è¦|å‹¾è‚¡å®šç†|çš„**è¯æ˜**|ã€**å˜ä½“|**æˆ–**å®é™…|åº”ç”¨ä¾‹å­**ï¼Œ|æˆ‘ä¹Ÿå¯ä»¥ç»§ç»­ä¸ºä½ |è¯¦ç»†è®²è§£ï¼||"
     ]
    }
   ],
   "source": [
    "for chunk in chat.stream(messages):\n",
    "    print(chunk.content, end=\"|\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e58683",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¼“å­˜"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3239680b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_llm_cache\n",
    "from langchain.cache import InMemoryCache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d99e259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 15.6 ms\n",
      "Wall time: 1.96 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='å½“ç„¶å¯ä»¥ï¼è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç¬‘è¯ï¼š\\n\\nä¸ºä»€ä¹ˆæ•°å­¦ä¹¦æ€»æ˜¯å¾ˆå¿§éƒï¼Ÿ  \\nå› ä¸ºå®ƒæœ‰å¤ªå¤šçš„é—®é¢˜ã€‚ ğŸ˜„\\n\\nä½ æƒ³å¬æ›´é•¿ä¸€ç‚¹çš„ï¼Œè¿˜æ˜¯æ¢ä¸ªç±»å‹çš„ï¼Ÿ', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'b4468d10-dc13-9b50-9ab0-d3450d6be05d', 'token_usage': {'input_tokens': 15, 'output_tokens': 35, 'total_tokens': 50, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--1b20956e-ae4b-4823-847f-1e014da6d60c-0')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "set_llm_cache(InMemoryCache())\n",
    "\n",
    "# The first time, it is not yet in cache, so it should take longer\n",
    "chat.invoke(\"è¯´ä¸€ä¸ªç¬‘è¯\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57e64381",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 31.2 ms\n",
      "Wall time: 626 Î¼s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "AIMessage(content='å½“ç„¶å¯ä»¥ï¼è¿™æ˜¯ä¸€ä¸ªç®€å•çš„ç¬‘è¯ï¼š\\n\\nä¸ºä»€ä¹ˆæ•°å­¦ä¹¦æ€»æ˜¯å¾ˆå¿§éƒï¼Ÿ  \\nå› ä¸ºå®ƒæœ‰å¤ªå¤šçš„é—®é¢˜ã€‚ ğŸ˜„\\n\\nä½ æƒ³å¬æ›´é•¿ä¸€ç‚¹çš„ï¼Œè¿˜æ˜¯æ¢ä¸ªç±»å‹çš„ï¼Ÿ', additional_kwargs={}, response_metadata={'model_name': 'qwen-turbo', 'finish_reason': 'stop', 'request_id': 'b4468d10-dc13-9b50-9ab0-d3450d6be05d', 'token_usage': {'input_tokens': 15, 'output_tokens': 35, 'total_tokens': 50, 'prompt_tokens_details': {'cached_tokens': 0}}}, id='run--1b20956e-ae4b-4823-847f-1e014da6d60c-0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "chat.invoke(\"è¯´ä¸€ä¸ªç¬‘è¯\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47aa5f0d",
   "metadata": {},
   "source": [
    "# è‡ªå®šä¹‰Chat model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8f2be6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b41af3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, AsyncIterator, Dict, Iterator, List, Optional\n",
    "\n",
    "from langchain_core.callbacks import (\n",
    "    AsyncCallbackManagerForLLMRun,\n",
    "    CallbackManagerForLLMRun,\n",
    ")\n",
    "from langchain_core.language_models import BaseChatModel, SimpleChatModel\n",
    "from langchain_core.messages import AIMessageChunk, BaseMessage, HumanMessage\n",
    "from langchain_core.outputs import ChatGeneration, ChatGenerationChunk, ChatResult\n",
    "from langchain_core.runnables import run_in_executor\n",
    "from langchain_core.messages import AIMessage, HumanMessage\n",
    "class CustomChatModelAdvanced(BaseChatModel):\n",
    "    \"\"\"A custom chat model that echoes the first `n` characters of the input.\n",
    "\n",
    "    When contributing an implementation to LangChain, carefully document\n",
    "    the model including the initialization parameters, include\n",
    "    an example of how to initialize the model and include any relevant\n",
    "    links to the underlying models documentation or API.\n",
    "\n",
    "    Example:\n",
    "\n",
    "        .. code-block:: python\n",
    "\n",
    "            model = CustomChatModel(n=2)\n",
    "            result = model.invoke([HumanMessage(content=\"hello\")])\n",
    "            result = model.batch([[HumanMessage(content=\"hello\")],\n",
    "                                 [HumanMessage(content=\"world\")]])\n",
    "    \"\"\"\n",
    "\n",
    "    model_name: str\n",
    "    \"\"\"The name of the model\"\"\"\n",
    "    n: int\n",
    "    \"\"\"The number of characters from the last message of the prompt to be echoed.\"\"\"\n",
    "\n",
    "    def _generate(\n",
    "            self,\n",
    "            messages: List[BaseMessage],\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> ChatResult:\n",
    "        \n",
    "        \"\"\"Override the _generate method to implement the chat model logic.\n",
    "\n",
    "        This can be a call to an API, a call to a local model, or any other\n",
    "        implementation that generates a response to the input prompt.\n",
    "\n",
    "        Args:\n",
    "            messages: the prompt composed of a list of messages.\n",
    "            stop: a list of strings on which the model should stop generating.\n",
    "                  If generation stops due to a stop token, the stop token itself\n",
    "                  SHOULD BE INCLUDED as part of the output. This is not enforced\n",
    "                  across models right now, but it's a good practice to follow since\n",
    "                  it makes it much easier to parse the output of the model\n",
    "                  downstream and understand why generation stopped.\n",
    "            run_manager: A run manager with callbacks for the LLM.\n",
    "        \"\"\"\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.cotent[:self.n]\n",
    "        message = AIMessage(\n",
    "            content=tokens,\n",
    "            additional_kwargs={},  # Used to add additional payload (e.g., function calling request)\n",
    "            response_metadata={  # Use for response metadata\n",
    "                \"time_in_seconds\": 3,\n",
    "            },\n",
    "        )\n",
    "\n",
    "        generation = ChatGeneration(message=message)\n",
    "        return ChatResult(generations=[generation]) # è¿”å›å¯¹è±¡\n",
    "    \n",
    "    def _stream(\n",
    "            self,\n",
    "            messages: List[BaseMessage],\n",
    "            stop: Optional[List[str]] = None,\n",
    "            run_manager: Optional[CallbackManagerForLLMRun] = None,\n",
    "            **kwargs: Any,\n",
    "    ) -> Iterator[ChatGenerationChunk]:\n",
    "        \"\"\"Override the _stream method to implement the chat model logic.\n",
    "        \"\"\"\n",
    "        last_message = messages[-1]\n",
    "        tokens = last_message.content[:self.n]\n",
    "\n",
    "        for token in tokens:\n",
    "            chunk = ChatGenerationChunk(message=AIMessageChunk(content=token))\n",
    "            yield chunk\n",
    "              # Let's add some other information (e.g., response metadata)\n",
    "        chunk = ChatGenerationChunk(\n",
    "            message=AIMessageChunk(content=\"\", response_metadata={\"time_in_sec\": 3})\n",
    "        )\n",
    "        if run_manager:\n",
    "            # This is optional in newer versions of LangChain\n",
    "            # The on_llm_new_token will be called automatically\n",
    "            run_manager.on_llm_new_token(token, chunk=chunk)\n",
    "        yield chunk\n",
    "\n",
    "    @property\n",
    "    def _llm_type(self) -> str:\n",
    "        \"\"\"Get the type of language model used by this chat model.\"\"\"\n",
    "        return \"echoing-chat-model-advanced\"\n",
    "\n",
    "    @property\n",
    "    def _identifying_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Return a dictionary of identifying parameters.\n",
    "\n",
    "        This information is used by the LangChain callback system, which\n",
    "        is used for tracing purposes make it possible to monitor LLMs.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            # The model name allows users to specify custom token counting\n",
    "            # rules in LLM monitoring applications (e.g., in LangSmith users\n",
    "            # can provide per token pricing for their model and monitor\n",
    "            # costs for the given LLM.)\n",
    "            \"model_name\": self.model_name,\n",
    "        }\n",
    "\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2e09b972",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HumanMessage' object has no attribute 'cotent'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 12\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_core\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmessages\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m      2\u001b[0m     AIMessage,\n\u001b[0;32m      3\u001b[0m     BaseMessage,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      7\u001b[0m     ToolMessage,\n\u001b[0;32m      8\u001b[0m )\n\u001b[0;32m     10\u001b[0m model \u001b[38;5;241m=\u001b[39m CustomChatModelAdvanced(n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, model_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmy_custom_model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 12\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhello!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mAIMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHi there human!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mHumanMessage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mMeow!\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:395\u001b[0m, in \u001b[0;36mBaseChatModel.invoke\u001b[1;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[0;32m    383\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    384\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21minvoke\u001b[39m(\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    390\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    391\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BaseMessage:\n\u001b[0;32m    392\u001b[0m     config \u001b[38;5;241m=\u001b[39m ensure_config(config)\n\u001b[0;32m    393\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(\n\u001b[0;32m    394\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChatGeneration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 395\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate_prompt(\n\u001b[0;32m    396\u001b[0m             [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_input(\u001b[38;5;28minput\u001b[39m)],\n\u001b[0;32m    397\u001b[0m             stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    398\u001b[0m             callbacks\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    399\u001b[0m             tags\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    400\u001b[0m             metadata\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    401\u001b[0m             run_name\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m    402\u001b[0m             run_id\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_id\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m    403\u001b[0m             \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    404\u001b[0m         )\u001b[38;5;241m.\u001b[39mgenerations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m],\n\u001b[0;32m    405\u001b[0m     )\u001b[38;5;241m.\u001b[39mmessage\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:980\u001b[0m, in \u001b[0;36mBaseChatModel.generate_prompt\u001b[1;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[0;32m    972\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[0;32m    973\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[0;32m    978\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[0;32m    979\u001b[0m     prompt_messages \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_messages() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[1;32m--> 980\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(prompt_messages, stop\u001b[38;5;241m=\u001b[39mstop, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:799\u001b[0m, in \u001b[0;36mBaseChatModel.generate\u001b[1;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[0;32m    796\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(input_messages):\n\u001b[0;32m    797\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    798\u001b[0m         results\u001b[38;5;241m.\u001b[39mappend(\n\u001b[1;32m--> 799\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate_with_cache(\n\u001b[0;32m    800\u001b[0m                 m,\n\u001b[0;32m    801\u001b[0m                 stop\u001b[38;5;241m=\u001b[39mstop,\n\u001b[0;32m    802\u001b[0m                 run_manager\u001b[38;5;241m=\u001b[39mrun_managers[i] \u001b[38;5;28;01mif\u001b[39;00m run_managers \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    803\u001b[0m                 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    804\u001b[0m             )\n\u001b[0;32m    805\u001b[0m         )\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    807\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m run_managers:\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\langchain-env\\lib\\site-packages\\langchain_core\\language_models\\chat_models.py:1045\u001b[0m, in \u001b[0;36mBaseChatModel._generate_with_cache\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m   1043\u001b[0m     result \u001b[38;5;241m=\u001b[39m generate_from_stream(\u001b[38;5;28miter\u001b[39m(chunks))\n\u001b[0;32m   1044\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m inspect\u001b[38;5;241m.\u001b[39msignature(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate)\u001b[38;5;241m.\u001b[39mparameters\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_manager\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m-> 1045\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(\n\u001b[0;32m   1046\u001b[0m         messages, stop\u001b[38;5;241m=\u001b[39mstop, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m   1047\u001b[0m     )\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(messages, stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "Cell \u001b[1;32mIn[8], line 59\u001b[0m, in \u001b[0;36mCustomChatModelAdvanced._generate\u001b[1;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[0;32m     43\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Override the _generate method to implement the chat model logic.\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;124;03mThis can be a call to an API, a call to a local model, or any other\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m    run_manager: A run manager with callbacks for the LLM.\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     58\u001b[0m last_message \u001b[38;5;241m=\u001b[39m messages[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 59\u001b[0m tokens \u001b[38;5;241m=\u001b[39m \u001b[43mlast_message\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcotent\u001b[49m[:\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn]\n\u001b[0;32m     60\u001b[0m message \u001b[38;5;241m=\u001b[39m AIMessage(\n\u001b[0;32m     61\u001b[0m     content\u001b[38;5;241m=\u001b[39mtokens,\n\u001b[0;32m     62\u001b[0m     additional_kwargs\u001b[38;5;241m=\u001b[39m{},  \u001b[38;5;66;03m# Used to add additional payload (e.g., function calling request)\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     65\u001b[0m     },\n\u001b[0;32m     66\u001b[0m )\n\u001b[0;32m     68\u001b[0m generation \u001b[38;5;241m=\u001b[39m ChatGeneration(message\u001b[38;5;241m=\u001b[39mmessage)\n",
      "File \u001b[1;32mc:\\Users\\Administrator\\.conda\\envs\\langchain-env\\lib\\site-packages\\pydantic\\main.py:991\u001b[0m, in \u001b[0;36mBaseModel.__getattr__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getattribute__\u001b[39m(item)  \u001b[38;5;66;03m# Raises AttributeError if appropriate\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    990\u001b[0m     \u001b[38;5;66;03m# this is the current error\u001b[39;00m\n\u001b[1;32m--> 991\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mitem\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'HumanMessage' object has no attribute 'cotent'"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    AIMessage,\n",
    "    BaseMessage,\n",
    "    FunctionMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage,\n",
    "    ToolMessage,\n",
    ")\n",
    "\n",
    "model = CustomChatModelAdvanced(n=3, model_name=\"my_custom_model\")\n",
    "\n",
    "model.invoke(\n",
    "    [\n",
    "        HumanMessage(content=\"hello!\"),\n",
    "        AIMessage(content=\"Hi there human!\"),\n",
    "        HumanMessage(content=\"Meow!\"),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb0526a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# è¾“å…¥ä¹Ÿæ”¯æŒå­—ç¬¦ä¸²ï¼Œå¯ä»¥ç­‰åŒäº`[HumanMessage(content=\"cat vs dog\")]`\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mstream(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat vs dog\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m(chunk\u001b[38;5;241m.\u001b[39mcontent, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "# è¾“å…¥ä¹Ÿæ”¯æŒå­—ç¬¦ä¸²ï¼Œå¯ä»¥ç­‰åŒäº`[HumanMessage(content=\"cat vs dog\")]`\n",
    "for chunk in model.stream(\"cat vs dog\"):\n",
    "    print(chunk.content, end=\"|\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
