{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96b98617",
   "metadata": {},
   "source": [
    "Rag From Scratch: Query Transformations\n",
    "Query transformations are a set of approaches focused on re-writing and / or modifying questions for retrieval.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c51f2571",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.embeddings import DashScopeEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "from langchain_community.embeddings import DashScopeEmbeddings \n",
    "\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "import bs4\n",
    "from langchain import hub\n",
    "import os\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5159a8a",
   "metadata": {},
   "source": [
    "Part 5: Multi Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "114f0301",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)\n",
    "\n",
    "\n",
    "embeding = DashScopeEmbeddings(model=\"text-embedding-v1\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embeding)\n",
    "\n",
    "\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4fd8955",
   "metadata": {},
   "source": [
    "##  promote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "880c7a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\", \n",
    "    model_provider=\"deepseek\",\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY')  # 添加这行\n",
    ")\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm \n",
    "    | StrOutputParser()   #  3. 解析输出为字符串\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2cd530e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_108584\\4144763748.py:19: LangChainBetaWarning: The function `loads` is in beta. It is actively being worked on, so the API may change.\n",
      "  return [loads(doc) for doc in unique_docs]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "# dumps: 将 LangChain Document 对象序列化为字符串\n",
    "# loads: 将字符串反序列化回 Document 对象\n",
    "# 得到 retrieval_results（多个文档列表的列表）\n",
    "# 假设有3个查询变体，每个返回不同的文档\n",
    "# retrieval_results = [\n",
    "#     [doc1, doc2, doc3],  # 查询1的结果\n",
    "#     [doc2, doc4, doc5],  # 查询2的结果  \n",
    "#     [doc1, doc6]         # 查询3的结果\n",
    "# ]\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "\n",
    "# Retrieval\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "\n",
    "# # 不使用 .map() 的情况\n",
    "# retriever.invoke(\"单个查询\")  # 返回：[doc1, doc2, doc3]\n",
    "\n",
    "# # 使用 .map() 的情况\n",
    "# retriever.map().invoke([\"查询1\", \"查询2\", \"查询3\"])\n",
    "# # 返回：[[doc1, doc2], [doc3, doc4], [doc5, doc6]]\n",
    "\n",
    "# retrieval_chain = (\n",
    "#     generate_queries           # 输入：单个问题\n",
    "#     | retriever.map()         # 输出：多个查询 → 多个文档列表\n",
    "#     | get_unique_union        # 输入：多个文档列表 → 输出：去重后的单个文档列表\n",
    "# )\n",
    "\n",
    "docs = retrieval_chain.invoke({\"question\": question})\n",
    "len(docs)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1d6f11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Task decomposition for LLM (Large Language Model) agents refers to the process of breaking down complex tasks into smaller, more manageable subgoals or steps. This enables the agent to handle intricate tasks more efficiently by addressing them in a structured manner. \\n\\nKey aspects of task decomposition include:\\n1. **Chain of Thought (CoT)**: A prompting technique where the model is instructed to \"think step by step,\" decomposing hard tasks into simpler steps. This approach helps in interpreting the model\\'s reasoning process.\\n   \\n2. **Tree of Thoughts (ToT)**: An extension of CoT that explores multiple reasoning possibilities at each step, creating a tree-like structure of thoughts. This allows for more comprehensive exploration of solutions, using methods like breadth-first search (BFS) or depth-first search (DFS).\\n\\n3. **Methods of Decomposition**:\\n   - **Simple prompting**: Using instructions like \"Steps for XYZ.\\\\\\\\n1.\" or \"What are the subgoals for achieving XYZ?\"\\n   - **Task-specific instructions**: For example, \"Write a story outline\" for writing a novel.\\n   - **Human inputs**: Leveraging human guidance to break down tasks.\\n\\nTask decomposition is a critical component of planning in LLM-powered autonomous agents, allowing them to tackle complex problems systematically and improve their problem-solving capabilities.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\", \n",
    "    model_provider=\"deepseek\",\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY')  # 添加这行\n",
    ")\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "# 总结：\n",
    "# \"question\"：传递字符串字面量 \"question\"\n",
    "# itemgetter(\"question\")：传递一个函数，用于从输入数据中动态提取 question 字段的值\n",
    "# 目的：确保传递的是用户的实际问题内容，而不是字段名\n",
    "# 这是 LangChain 管道设计中处理动态数据提取的标准模式。\n",
    "final_rag_chain.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c5dd816",
   "metadata": {},
   "source": [
    "Part 6: RAG-Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b132e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template = \"\"\"You are a helpful assistant that generates multiple search queries based on a single input query. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (4 queries):\"\"\"\n",
    "prompt_rag_fusion = ChatPromptTemplate.from_template(template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb0dc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\", \n",
    "    model_provider=\"deepseek\",\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY')  # 添加这行\n",
    ")\n",
    "generate_queries = (\n",
    "    prompt_rag_fusion\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "# # LLM 生成的回答通常是这样的格式：\n",
    "# response = \"\"\"\n",
    "# 什么是机器学习？\n",
    "# 机器学习的定义是什么？\n",
    "# 如何理解机器学习？\n",
    "# 机器学习的基本概念\n",
    "# 机器学习是什么技术？\n",
    "# \"\"\"\n",
    "\n",
    "# 最终生成多个查询如：\n",
    "# [\"什么是机器学习？\", \"机器学习的定义是什么？\", \"如何理解机器学习？\", \"机器学习的基本概念\", \"机器学习是什么技术？\"]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8ba4d2f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reciprocal_rank_fusion算法\n",
    "from langchain.load import dumps, loads\n",
    "def reciprocal_rank_fusion(results: list[list], k=60):\n",
    "    \"\"\"\n",
    "    对已经排好序的多个文档列表[[doc1, doc2, doc3], [doc4, doc5, doc6], [doc7, doc8, doc9]]\n",
    "    进行融合排序，计算分数\n",
    "    \"\"\"\n",
    "    fusion_results = {}\n",
    "\n",
    "    for docs in results:\n",
    "        for i, doc in enumerate(docs):\n",
    "            doc_str = dumps(doc)\n",
    "            if doc_str not in fusion_results:\n",
    "                fusion_results[doc_str] = 0\n",
    "            \n",
    "            fusion_results[doc_str] += 1 / (i + k)\n",
    "    # 对fusion_results进行排序\n",
    "    sorted_results = sorted(fusion_results.items(), key=lambda x: x[1], reverse=True)\n",
    "    reanked_results = [(loads(doc),score) for doc,score in sorted_results]\n",
    "    return reanked_results\n",
    "\n",
    "retrieval_chain_rag_fusion = generate_queries | retriever.map() |reciprocal_rank_fusion\n",
    "docs = retrieval_chain_rag_fusion.invoke({\"question\": question})\n",
    "len(docs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea001f81",
   "metadata": {},
   "source": [
    "rag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116b165d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "final_rag_chain.invoke({\"question\":question})\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70464337",
   "metadata": {},
   "source": [
    "Part 7: Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "25015372",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3221d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\", \n",
    "    model_provider=\"deepseek\",\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY')  # 添加这行\n",
    ")\n",
    "\n",
    "# chain\n",
    "generate_queries_decomposition = (\n",
    "    prompt_decomposition\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "# Run\n",
    "question = \"What are the main components of an LLM-powered autonomous agent system?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "740d4f82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three sub-questions related to the main components of an LLM-powered autonomous agent system:  ',\n",
       " '',\n",
       " '1. **What are the key architectural components of an LLM-powered autonomous agent?**  ',\n",
       " '2. **How does memory and context management work in an LLM-based autonomous agent?**  ',\n",
       " '3. **What role do APIs and external tools play in an LLM autonomous agent system?**  ',\n",
       " '',\n",
       " 'These queries help break down the original question into more specific aspects of the system, such as architecture, memory handling, and integration with external tools.']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7922737b",
   "metadata": {},
   "source": [
    "HYDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f98bd11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**Task Decomposition for LLM Agents**  \\n\\nTask decomposition is a fundamental capability of Large Language Model (LLM)-based agents, enabling them to break down complex, high-level tasks into smaller, more manageable subtasks. This process enhances the agent\\'s ability to plan, reason, and execute tasks efficiently by addressing each component sequentially or in parallel, depending on the task structure.  \\n\\n### **Mechanisms of Task Decomposition**  \\nLLM agents employ various strategies for task decomposition, including:  \\n1. **Step-by-Step Breakdown**: The agent recursively divides a task into finer-grained steps, ensuring logical coherence and feasibility. For example, the instruction \"Plan a conference\" may be decomposed into subtasks such as \"Select a venue,\" \"Invite speakers,\" and \"Promote the event.\"  \\n2. **Hierarchical Planning**: The agent organizes tasks in a tree-like structure, where high-level goals branch into subgoals with dependencies. This is particularly useful in multi-stage problems like software development or project management.  \\n3. **Prompt-Based Guidance**: By leveraging structured prompts (e.g., Chain-of-Thought or Tree-of-Thought prompting), the agent generates intermediate reasoning steps that implicitly decompose the task.  \\n\\n### **Applications and Benefits**  \\nTask decomposition improves LLM agents\\' performance in:  \\n- **Complex Problem Solving**: By isolating subtasks, the agent can tackle intricate problems (e.g., mathematical proofs or coding challenges) with higher accuracy.  \\n- **Human-Agent Collaboration**: Decomposed tasks are more interpretable, allowing humans to verify, modify, or intervene in specific steps.  \\n- **Adaptive Execution**: Agents can dynamically adjust subtask prioritization based on real-time feedback or environmental changes.  \\n\\n### **Challenges and Future Directions**  \\nCurrent limitations include over-decomposition (leading to inefficiency) and dependency errors (misordering subtasks). Future research may explore hybrid neuro-symbolic methods for more robust decomposition and integration with external tools for verification.  \\n\\nIn summary, task decomposition is a critical cognitive function for LLM agents, bridging high-level intentions with actionable steps while enhancing scalability and reliability in real-world applications.  \\n\\n---  \\n*References*:  \\n- [1] Yao et al. (2023). \"Tree of Thoughts: Deliberate Problem Solving with Large Language Models.\"  \\n- [2] Zhou et al. (2022). \"Least-to-Most Prompting Enables Complex Reasoning in Large Language Models.\"'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "llm = init_chat_model(\n",
    "    model=\"deepseek-chat\", \n",
    "    model_provider=\"deepseek\",\n",
    "    api_key=os.getenv('DEEPSEEK_API_KEY')  # 添加这行\n",
    ")\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3f1fd256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 检索\n",
    "retriever_chain = (\n",
    "    generate_docs_for_retrieval\n",
    "    | retriever\n",
    ")\n",
    "\n",
    "# 运行检索链条\n",
    "retrieval_results = retriever_chain.invoke({\"question\": question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "68c5e28a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Component One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:'),\n",
       " Document(metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}, page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieval_results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556e5e78",
   "metadata": {},
   "source": [
    "RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ec5239b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain_rag_fusion, \"question\": itemgetter(\"question\")}|\n",
    "    prompt |\n",
    "    llm |\n",
    "    StrOutputParser())\n",
    "\n",
    "final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
